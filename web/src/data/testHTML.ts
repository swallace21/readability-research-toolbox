const testHTML = `<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Unidoc-3p</title><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 .s1 {vertical-align: 3pt;}
 .s2 {vertical-align: 6pt;}
 .s3 {vertical-align: 3pt;}
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 1; }
 #l1> li>*:first-child:before {counter-increment: c1; content: counter(c1, decimal)" ";  }
 #l1> li:first-child>*:first-child:before {counter-increment: c1 0;  }
 #l2 {padding-left: 0pt;counter-reset: c2 1; }
 #l2> li>*:first-child:before {counter-increment: c2; content: counter(c1, decimal)"."counter(c2, decimal)" ";  }
 #l2> li:first-child>*:first-child:before {counter-increment: c2 0;  }
</style></head><body><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 4pt;text-align: left;">
</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: center;">
UniDoc: Unified Pretraining Framework for Document Understanding</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 1pt;text-align: left;">
</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 85pt;text-indent: 0pt;text-align: center;">Jiuxiang Gu<span class="s1">1</span>, Jason Kuen<span class="s1">1</span>
, Vlad I. Morariu<span class="s1">1</span>, Handong Zhao<span class="s1">1</span>, Nikolaos Barmpalios<span class="s1">2</span>, Rajiv Jain<span class="s1">1</span>, Ani Nenkova<span class="s1">1</span>
, Tong Sun<span class="s1">1 </span><span class="s2">1</span><span class="p">Adobe Research, </span><span class="s3">2</span><span class="p">Adobe Document Cloud</span></h3>
<p class="s4" style="padding-left: 28pt;text-indent: 0pt;text-align: center;">{jigu,kuen,morariu,hazhao,barmpali,rajijain,nenkova,tsun}@adobe.com</p><p style="text-indent: 0pt;text-align: left;"><br/></p>
<h2 style="padding-left: 28pt;text-indent: 0pt;text-align: center;">Abstract</h2><p style="padding-top: 12pt;padding-left: 57pt;text-indent: 0pt;">Document intelligence automates the extraction of information from documents and supports many business applications. Recent self-supervised learning methods on large-scale unlabeled document datasets have opened up promising directions towards reducing annotation efforts by training models with self-supervised ob- jectives. However, most of the existing document pretraining methods are still language-dominated. We present UniDoc, a new unified pretraining framework for document understanding. UniDoc is designed to support most document under- standing tasks, extending the Transformer to take multimodal embeddings as input. Each input element is composed of words and visual features from a semantic re- gion of the input document image. An important feature of UniDoc is that it learns a generic representation by making use of three self-supervised losses, encourag- ing the representation to model sentences, learn similarities, and align modalities. Extensive empirical analysis demonstrates that the pretraining procedure learns better joint representations and leads to improvements in downstream tasks.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l1"><li><h2 style="padding-top: 7pt;padding-left: 39pt;text-indent: -17pt;text-align: left;">Introduction</h2><p style="padding-top: 12pt;padding-left: 21pt;text-indent: 0pt;">Document intelligence is a broad research area that includes techniques for information extraction and understanding. Unlike plain-text documents in natural language processing (NLP) [<span style=" color: #0F0;">1</span>, <span style=" color: #0F0;">2</span>, <span style=" color: #0F0;">3</span>], a physical document can be composed of multiple elements: tables, figures, charts, <i>etc</i>. In addition, a document usually includes rich visual information, and can be one of various types of documents (scientific paper, form, resume, <i>etc</i>.), with various combinations of multiple elements and layouts. Complex content and layout, noisy data, font and style variations make automatic document understanding very challenging. For example, to understand text-rich documents such as letters, a system needs to focus almost exclusively on text content, paying attention to a long sequential context, while processing semi-structured documents such as forms requires the system to analyze spatially distributed short words, paying particular attention to the spatial arrangement of the words. Following the success of BERT [<span style=" color: #0F0;">4</span>] on NLP tasks, there has been growing interest in developing pretraining methods for document understanding [<span style=" color: #0F0;">5</span>, <span style=" color: #0F0;">6</span>, <span style=" color: #0F0;">7</span>, <span style=" color: #0F0;">8</span>]. Pretrained models have achieved state-of-the-art (SoTA) performance across diverse document understanding tasks [<span style=" color: #0F0;">9</span>, <span style=" color: #0F0;">10</span>].</p><p style="padding-top: 5pt;padding-left: 22pt;text-indent: 0pt;">Huge training datasets help pretraining models to learn a good representation for downstream tasks, however, we observe three major problems with the current pretraining setup: <i>(1) documents are composed of semantic regions</i>. Most of the recent document pretraining works follow BERT and split documents into words. However, unlike the sequence-to-sequence learning in NLP, documents have a hierarchical structure (words form sentences, sentences form a semantic region, and semantic regions form a document). Also, the importance of words and sentences are highly context-dependent, <i>i.e</i>., the same word or sentence may have different importance in a different context. Moreover, current transformer-based document pretraining models suffer from input length constraints. Also, input</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 22pt;text-indent: 0pt;">Submitted to 35th Conference on Neural Information Processing Systems (NeurIPS 2021). Do not distribute.</p><p style="padding-top: 4pt;padding-left: 21pt;text-indent: 0pt;">length becomes a problem for text-rich documents or multi-page documents. <i>(2) documents are more than words</i>. The semantic structure of the document is not only determined by the text within it but also the visual features such as table, font size and style, and figure, <i>etc</i>. Moreover, the visual appearance of the text within a block are often overlooked. Most of recent BERT-based pre- training works only take the words as input without considering multimodal content and alignment of multimodal information within semantic regions. <i>(3) documents have spatial layout</i>. Visual and layout information is critical for document understanding. Recent works encode spatial information via 2D position encoding and model spatial relationships with self-attention, which computes attention weights for long inputs [<span style=" color: #0F0;">5</span>, <span style=" color: #0F0;">6</span>]. However, for semi-structured documents, such as forms and receipts, words are more related to their local surroundings. This corresponds strongly with human intuition – when we look at magazines or newspapers, the receptive fields are modulated by our reading order and attention. Based on the above observations, we ask the following question: can unified document pretraining benefit all of these different kinds of documents?</p><p style="padding-top: 5pt;padding-left: 22pt;text-indent: 0pt;">We propose a unified pretraining framework for document understanding, shown in Fig. <span style=" color: #F00;">1</span>. Our model integrates image information in the pretraining stage by taking advantage of the transformer architecture to learn cross-modal interactions between visual and textual information. To handle textual information, we encode sentences using a hierarchical transformer encoder. The first level of the hierarchical encoder models the formation of the sentences from words. The second level models the formation of the document from sentences. With the help of the hierarchical structure, UniDoc learns how words form sentences and how sentences form documents. Meanwhile, it reduces model computation complexity exponentially and increases the number of input words. This also mimics human reading behaviors since the sentence/paragraph is a reasonable unit for people to read and understand—people rarely check the interactions between arbitrary words across different regions in order to understand an article. Convolution has been very successful in the extraction of local features that encode visual and spatial information [<span style=" color: #0F0;">11</span>], so we use convolution layers as a more efficient complement to self-attention for addressing local intra-region dependencies in a document image. Meanwhile, self-attention uses all input tokens to generate attention weights for capturing global dependencies. Thus, we combine convolution with self-attention to form a mixed attention mechanism that combines the advantages of the two operations.</p><p style="padding-top: 5pt;padding-left: 21pt;text-indent: 0pt;">We depart from previous vision-language pretraining [<span style=" color: #0F0;">12</span>, <span style=" color: #0F0;">13</span>] by extracting both the textual and visual features for each semantic region. We propose a novel gated cross-attentional transformer that enables information exchange between modalities. A visually-rich region (figure, chart, <i>etc</i>) may have stronger visual information than textual information. Instead of treating outputs from both modalities identically, we design a gating mechanism that can dynamically control the influence of textual and visual features. This approach enables cross-modal connections and allows for variable highlight the relevant information in visual and textual modality and enables cross-modal connections. During pretraining, the CNN-based visual backbone and multi-layer gated cross-attention encoder are jointly trained in both pretraining and fine-tuning phase.</p><p style="padding-top: 5pt;padding-left: 21pt;text-indent: 0pt;">Our contributions are summarized as follows: (1) We introduce UniDoc, a powerful pretraining framework for document understanding. UniDoc is capable of learning contextual textual and visual information and cross-modal correlations within a single framework, which leads to better performance. (2) We present Masked Sentence Modeling for language modeling, Visual Contrastive Learning for vision modeling, and Vision-Language Alignment for pretraining. (3) We present extensive experiments and analyses to validate the effectiveness of the proposed UniDoc. Extensive experiments and analysis provide useful insights on the effectiveness of the pretraining tasks and show outstanding performance on various downstream tasks.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><h2 style="padding-top: 9pt;padding-left: 39pt;text-indent: -17pt;text-align: left;">Related Work</h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 21pt;text-indent: 0pt;">Self-supervised learning has shown great success in producing generic representations that learn from large-scale unlabeled corpora [<span style=" color: #0F0;">4</span>]. Like the development of pretraining in computer vision [<span style=" color: #0F0;">14</span>] and NLP [<span style=" color: #0F0;">4</span>], there has been a surging interest in self-supervised learning for Vision-Language (VL) tasks [<span style=" color: #0F0;">12</span>, <span style=" color: #0F0;">15</span>, <span style=" color: #0F0;">13</span>, <span style=" color: #0F0;">16</span>, <span style=" color: #0F0;">17</span>, <span style=" color: #0F0;">18</span>]. Transformers [<span style=" color: #0F0;">4</span>] are the key technology that enables learning contextualized representations from large-scale unlabeled training data. These methods generally employ BERT-like objectives to learn cross-modal representations from visual region features and word embeddings. A major difference between natural images and document images is that documents</p><p style="text-indent: 0pt;text-align: left;">
</p><p style="text-indent: 0pt;text-align: left;"></p><p style="padding-top: 7pt;padding-left: 22pt;text-indent: 0pt;">Figure 1: Overview of the proposed approach, UniDoc. UniDoc first uses a CNN-based visual backbone to learn visual representations. The model then extracts the RoI features with OCR bounding boxes and generates a multimodal embedding by combining the textual embedding and position encoding. The transformer-based encoder takes a set of masked multimodal embeddings as input and is pretrained with three pretraining tasks. All the network parameters except those of the textual encoder are jointly trained during both pretraining and fine-tuning phases are designed by humans. Unlike the image-caption pairs used in VL, document images do not have captions, and the visual and textual information are co-occur within semantic regions.</p><p style="padding-top: 5pt;padding-left: 21pt;text-indent: 0pt;">Several recent works have explored pretraining on document images [<span style=" color: #0F0;">5</span>, <span style=" color: #0F0;">6</span>, <span style=" color: #0F0;">19</span>, <span style=" color: #0F0;">20</span>]. LayoutLM [<span style=" color: #0F0;">5</span>] extends BERT to learn contextualized word representations for document images through multi-task learning. It takes a sequence of OCR words as input during pretraining and incorporates the 2D position embedding as input for each token. However, LayoutLM only considers textual information during pretraining without modeling the alignment between visual and textual information–visual information is only incorporated into the model during the fine-tuning stage. The most recent version, LayoutLMv2 [<span style=" color: #0F0;">6</span>], improves on this by incorporating the image encoder into pretraining and jointly training the image encoder along with the BERT model. LayoutLMv2 splits the document image into several parts and concatenates the visual embeddings and text embeddings into a single sequence. Apart from masked token learning for language, LayoutLMv2 also considers image-text alignment and image-text matching during pretraining. LAMPERT [<span style=" color: #0F0;">8</span>] proposes a structure that exploits pretraining tasks to learn a hierarchical framework. It uses the English Wikipedia pages as the target dataset, which provides the parsed content blocks. The most related work to ours is SelfDoc [<span style=" color: #0F0;">7</span>], which proposes a multimodal document pretraining framework. It first extracts the document object proposals from pre-trained Faster R-CNN [<span style=" color: #0F0;">21</span>] and then applies OCR for each proposal to get the words. It takes the pre-extracted RoI features and sentence embeddings as input, and models the perform learning over the textual and visual information using the cross-modality encoder.</p><p style="padding-top: 5pt;padding-left: 22pt;text-indent: 0pt;">There is a noticeable difference between our proposed method, UniDoc, and other concurrent works in document pretraining. Unlike the fixed document object detector in [<span style=" color: #0F0;">7</span>], the parameters of the image encoder with RoI align, which derive the visual features for semantic regions, are also updated in UniDoc. In contrast to [<span style=" color: #0F0;">6</span>], our visual features come from the semantic regions instead of splitting the image into fixed regions. Moreover, to learn the contextualized visual representations, UniDoc masks visual information in the latent space and learns contextualized representations by solving a contrastive learning task defined over a quantization of the latent visual embeddings.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><h2 style="padding-left: 39pt;text-indent: -17pt;text-align: left;">Method</h2><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l2"><li><h3 style="padding-left: 44pt;text-indent: -22pt;text-align: left;">Model Architecture</h3></li></ol></li></ol><p style="padding-top: 10pt;padding-left: 21pt;text-indent: 0pt;">Fig. <span style=" color: #F00;">1 </span>illustrates our approach, UniDoc, which consists of four components: feature extraction, feature embedding, multi-layer gated cross-attention encoder, and pretraining tasks. Given a document image and the locations of document elements (sentence or RoI), UniDoc takes image regions and words that correspond to each document elements as inputs, and extracts their respective embeddings through a visual feature extractor and a sentence encoder. These embeddings are then fed into a transformer-based encoder to learn the cross-modal contextualized embeddings that integrate both visual features and textual features.</p></body></html>`

export default testHTML;
/*
 h1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 17pt; }
 h3 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s1 { color: black; font-family:"Eras Medium ITC", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s2 { color: black; font-family:"Eras Medium ITC", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 6pt; }
 .p, p { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; margin:0pt; }
 .s3 { color: black; font-family:"Eras Medium ITC", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 3pt; }
 .s4 { color: black; font-family:"Courier New", monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 h2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s7 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }

 */